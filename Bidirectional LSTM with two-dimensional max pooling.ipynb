{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Reshape, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"reviews_Home_and_Kitchen_5.json\"]\n",
    "filename = \"reviews_Home_and_Kitchen_5.json\"\n",
    "t = Tokenizer()\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "\n",
    "dictionary_size = 0\n",
    "embed_dim = 300\n",
    "num_hidden_unit_lstm = 300\n",
    "num_filter = 100\n",
    "window_size = 3\n",
    "pool_size = 2\n",
    "batch_size = 10\n",
    "learning_rate = 0.1\n",
    "#AdaDelta\n",
    "dropout_embed = 0.5\n",
    "dropout_blstm = 0.2\n",
    "dropout_penultimate = 0.4\n",
    "l2_lambda = 0.000001\n",
    "\n",
    "randomShuffle = ShuffleSplit(n_splits=1, test_size=.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-97e30d809f62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"frame\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    462\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m             obj = self._get_object_parser(\n\u001b[1;32m--> 464\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m             )\n\u001b[0;32m    466\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'frame'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'series'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    804\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m             self.obj = DataFrame(\n\u001b[1;32m--> 806\u001b[1;33m                 loads(json, precise_float=self.precise_float), dtype=None)\n\u001b[0m\u001b[0;32m    807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_process_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "data = pd.read_json(filename, lines=True, orient=\"frame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['overall'] = np.where(data['overall'] < 3, 0, data.overall)\n",
    "data['overall'] = np.where(data['overall'] == 3, 1, data.overall)\n",
    "data['overall'] = np.where(data['overall'] > 3, 2, data.overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = list(data.iloc[:, 2].values)\n",
    "for j in range(3):\n",
    "    print(str(j) + \": \" + str(label.count(j)) + \"\\t\\t\" + str(round(label.count(j) / len(label) * 100, 2)) + \"%\")\n",
    "\n",
    "plt.plot([0, 1, 2], [label.count(0), label.count(1), label.count(2)], 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingMatrix(vocabulary_size):\n",
    "    embeddings_index = dict()\n",
    "    f = open('glove.6b/glove.6B.300d.txt', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    embedding_matrix = np.zeros((vocabulary_size, embed_dim))\n",
    "    for word, i in t.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 classes(0: 1-2 star, 1: 4-5 star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXY(filename, num_class):\n",
    "    data = pd.read_json(filename, lines=True, orient=\"frame\")\n",
    "    data['overall'] = np.where(data['overall'] < 3, 0, data.overall)\n",
    "    data['overall'] = np.where(data['overall'] > 3, 1, data.overall)\n",
    "    \n",
    "    # down sample\n",
    "    data_0 = data.loc[data['overall'] == 0]\n",
    "    data_1 = data.loc[data['overall'] == 1].sample(len(data_0))\n",
    "    \n",
    "    data = data_0.append(data_1).sample(frac=1)\n",
    "    \n",
    "    good_columns = [3]\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    data_X = data.iloc[:, good_columns]\n",
    "    data_Y = data.iloc[:, 2]\n",
    "    \n",
    "    reviewTextList = data.reviewText.values\n",
    "    t.fit_on_texts(reviewTextList)\n",
    "    text = t.texts_to_sequences(reviewTextList)\n",
    "    text = sequence.pad_sequences(text, maxlen=maxlen)\n",
    "    print('text shape:', text.shape)\n",
    "    vocabulary_size = len(t.word_index) + 1\n",
    "    print('vocabulary size:', vocabulary_size)\n",
    "    \n",
    "    data_X = np.array(text)\n",
    "    data_Y = np_utils.to_categorical(data_Y, num_class)\n",
    "    return data_X, data_Y, vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectionalLSTMCNNModel(containsGlove, num_class, str_loss, x_train, x_test, y_train, y_test, batch_size, epochs, vocabulary_size, embedding_matrix, name_model):\n",
    "    print(\"x_train: \", x_train.shape)\n",
    "    print(\"y_train: \", y_train.shape)\n",
    "    print(\"x_test: \", x_test.shape)\n",
    "    print(\"y_test: \", y_test.shape)\n",
    "#     print(\"x_train:\",x_train[0])\n",
    "#     print(\"x_test:\",x_test[0])\n",
    "#     print('y_train:',y_train[0])\n",
    "#     print('y_test',y_test[0])\n",
    "    model = Sequential()\n",
    "    print(\"vocabulary_size:\", vocabulary_size)\n",
    "    if (containsGlove == False):\n",
    "        model.add(Embedding(input_dim=vocabulary_size, output_dim=embed_dim, input_length=maxlen))\n",
    "    else:\n",
    "        model.add(Embedding(input_dim=vocabulary_size, output_dim=embed_dim, input_length=maxlen, weights=[embedding_matrix], trainable=False))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            LSTM(\n",
    "                num_hidden_unit_lstm, \n",
    "                return_sequences=True, \n",
    "                 kernel_regularizer=l2(l2_lambda)\n",
    "            ), \n",
    "            merge_mode=\"sum\"\n",
    "        ))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Reshape((maxlen, embed_dim, 1)))\n",
    "    model.add(Conv2D(num_filter,\n",
    "                     kernel_size=(3,3),\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1,\n",
    "                      kernel_regularizer=l2(l2_lambda)\n",
    "                    ))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_class, kernel_regularizer=l2(l2_lambda)))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    print(model.summary())\n",
    "    model.compile(loss=str_loss, \n",
    "                  optimizer='Adadelta',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print('Train...')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=[x_test, y_test])\n",
    "    \n",
    "    score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    print('Test score:', score)\n",
    "    print('Test accuracy:', acc)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    y_pred = model.predict_classes(x_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    model.save(name_model)\n",
    "    del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare without and with GloVe Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Home_and_Kitchen_5.json:\n",
      "text shape: (102838, 100)\n",
      "vocabulary size: 72649\n",
      "Loaded 400000 word vectors.\n",
      "x_train:  (82270, 100)\n",
      "y_train:  (82270, 2)\n",
      "x_test:  (20568, 100)\n",
      "y_test:  (20568, 2)\n",
      "vocabulary_size: 72649\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          21794700  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 300)          1442400   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 100, 300, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 98, 298, 100)      1000      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 49, 149, 100)      0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 49, 149, 100)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 730100)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1460202   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 24,698,302\n",
      "Trainable params: 24,698,302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train...\n",
      "Train on 82270 samples, validate on 20568 samples\n",
      "Epoch 1/1\n",
      "82270/82270 [==============================] - 3357s 41ms/step - loss: 0.3445 - acc: 0.8422 - val_loss: 0.2613 - val_acc: 0.8894\n",
      "20568/20568 [==============================] - 251s 12ms/step\n",
      "Test score: 0.2612825062097666\n",
      "Test accuracy: 0.8893669706195859\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.94      0.89     10248\n",
      "          1       0.93      0.84      0.88     10320\n",
      "\n",
      "avg / total       0.89      0.89      0.89     20568\n",
      "\n",
      "x_train:  (82270, 100)\n",
      "y_train:  (82270, 2)\n",
      "x_test:  (20568, 100)\n",
      "y_test:  (20568, 2)\n",
      "vocabulary_size: 72649\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 300)          21794700  \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 100, 300)          1442400   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 100, 300, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 98, 298, 100)      1000      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 49, 149, 100)      0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 49, 149, 100)      0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 730100)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1460202   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 24,698,302\n",
      "Trainable params: 2,903,602\n",
      "Non-trainable params: 21,794,700\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train...\n",
      "Train on 82270 samples, validate on 20568 samples\n",
      "Epoch 1/1\n",
      "82270/82270 [==============================] - 2917s 35ms/step - loss: 0.3934 - acc: 0.8224 - val_loss: 0.3200 - val_acc: 0.8733\n",
      "20568/20568 [==============================] - 252s 12ms/step\n",
      "Test score: 0.32004917558581103\n",
      "Test accuracy: 0.8732740111918544\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.81      0.86     10248\n",
      "          1       0.83      0.94      0.88     10320\n",
      "\n",
      "avg / total       0.88      0.87      0.87     20568\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 0\n",
    "num_class = 2\n",
    "for filename in filenames:\n",
    "    print(\"-----------------------------\")\n",
    "    print(filename + \":\")\n",
    "    X, Y, vocabulary_size = getXY(filename, num_class)\n",
    "    embedding_matrix = getEmbeddingMatrix(vocabulary_size)\n",
    "    for train_index, test_index in randomShuffle.split(X):\n",
    "        X_train = X[train_index]\n",
    "        Y_train = Y[train_index, :]\n",
    "        X_test = X[test_index]\n",
    "        Y_test = Y[test_index, :]\n",
    "        \n",
    "        bidirectionalLSTMCNNModel(False, num_class, 'categorical_crossentropy', X_train, X_test, Y_train, Y_test, 10, 1, vocabulary_size, embedding_matrix, filename + 'BLSTM+CNN_without.h5')\n",
    "        bidirectionalLSTMCNNModel(True, num_class, 'categorical_crossentropy', X_train, X_test, Y_train, Y_test, 10, 1, vocabulary_size, embedding_matrix, filename + 'BLSTM+CNN_with.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run without GloVe for 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "reviews_Home_and_Kitchen_5.json:\n",
      "text shape: (102838, 100)\n",
      "vocabulary size: 72038\n",
      "Loaded 400000 word vectors.\n",
      "x_train:  (82270, 100)\n",
      "y_train:  (82270, 2)\n",
      "x_test:  (20568, 100)\n",
      "y_test:  (20568, 2)\n",
      "vocabulary_size: 72038\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          21611400  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 300)          1442400   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 100, 300, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 98, 298, 100)      1000      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 49, 149, 100)      0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 49, 149, 100)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 730100)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1460202   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 24,515,002\n",
      "Trainable params: 24,515,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train...\n",
      "Train on 82270 samples, validate on 20568 samples\n",
      "Epoch 1/5\n",
      "82270/82270 [==============================] - 4542s 55ms/step - loss: 0.3372 - acc: 0.8478 - val_loss: 0.2626 - val_acc: 0.8910\n",
      "Epoch 2/5\n",
      "82270/82270 [==============================] - 4121s 50ms/step - loss: 0.2419 - acc: 0.9018 - val_loss: 0.3047 - val_acc: 0.8986\n",
      "Epoch 3/5\n",
      "82270/82270 [==============================] - 4070s 49ms/step - loss: 0.2127 - acc: 0.9166 - val_loss: 0.2665 - val_acc: 0.9070\n",
      "Epoch 4/5\n",
      "82270/82270 [==============================] - 4071s 49ms/step - loss: 0.1919 - acc: 0.9270 - val_loss: 0.2288 - val_acc: 0.9159\n",
      "Epoch 5/5\n",
      "82270/82270 [==============================] - 4094s 50ms/step - loss: 0.1743 - acc: 0.9356 - val_loss: 0.2614 - val_acc: 0.9109\n",
      "20568/20568 [==============================] - 218s 11ms/step\n",
      "Test score: 0.261427142440357\n",
      "Test accuracy: 0.9108809721882545\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.91      0.91     10379\n",
      "          1       0.91      0.91      0.91     10189\n",
      "\n",
      "avg / total       0.91      0.91      0.91     20568\n",
      "\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "`save_model` requires h5py.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-a0b065e6bfc7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mbidirectionalLSTMCNNModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'BLSTM+CNN.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-d78c061e9655>\u001b[0m in \u001b[0;36mbidirectionalLSTMCNNModel\u001b[1;34m(containsGlove, num_class, str_loss, x_train, x_test, y_train, y_test, batch_size, epochs, vocabulary_size, embedding_matrix, name_model)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m   2578\u001b[0m         \"\"\"\n\u001b[0;32m   2579\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2580\u001b[1;33m         \u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2582\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'`save_model` requires h5py.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_json_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: `save_model` requires h5py."
     ]
    }
   ],
   "source": [
    "vocabulary_size = 0\n",
    "num_class = 2\n",
    "for filename in filenames:\n",
    "    print(\"-----------------------------\")\n",
    "    print(filename + \":\")\n",
    "    X, Y, vocabulary_size = getXY(filename, num_class)\n",
    "    for train_index, test_index in randomShuffle.split(X):\n",
    "        X_train = X[train_index]\n",
    "        Y_train = Y[train_index, :]\n",
    "        X_test = X[test_index]\n",
    "        Y_test = Y[test_index, :]\n",
    "        \n",
    "        bidirectionalLSTMCNNModel(False, num_class, 'categorical_crossentropy', X_train, X_test, Y_train, Y_test, 10, 5, vocabulary_size, embedding_matrix, 'BLSTM+CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4fe82339dafa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'new.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.save('new.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('BLSTM+CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXY(filename, num_class):\n",
    "    cat = [0, 1]\n",
    "    def cat_y(y):\n",
    "        if y<=2.0:\n",
    "            return cat[0]\n",
    "        elif y>=4.0:\n",
    "            return cat[1]\n",
    "\n",
    "    def get_reviews(path, n_samples):\n",
    "        dt = {}\n",
    "        i=0\n",
    "        with open(path) as f:\n",
    "            for d in f.readlines():\n",
    "                dt[i] = eval(d)\n",
    "                i += 1\n",
    "        df = pd.DataFrame.from_dict(dt, orient='index')[['reviewText','overall']]\n",
    "        print(\"before: \", len(df))\n",
    "        df = df.drop(df[df.overall == 3.0].index)\n",
    "        print(\"after: \", len(df))\n",
    "        df = df[df['reviewText'].apply(lambda x: len(x.split())>=45)]\n",
    "        df['bucket'] = df['overall'].apply(cat_y)\n",
    "\n",
    "        df = df.groupby('bucket').apply(lambda x: x.sample(n=n_samples))\n",
    "        return df\n",
    "\n",
    "    data = get_reviews('data/reviews_Home_and_Kitchen_5.json', 10000)\n",
    "    \n",
    "    good_columns = [3]\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    data_X = data.iloc[:, 0]\n",
    "    data_Y = data.iloc[:, 2]\n",
    "    \n",
    "    reviewTextList = data.reviewText.values\n",
    "    t.fit_on_texts(reviewTextList)\n",
    "    text = t.texts_to_sequences(reviewTextList)\n",
    "    text = sequence.pad_sequences(text, maxlen=maxlen)\n",
    "    print('text shape:', text.shape)\n",
    "    vocabulary_size = len(t.word_index) + 1\n",
    "    print('vocabulary size:', vocabulary_size)\n",
    "    \n",
    "    data_X = np.array(text)\n",
    "    data_Y = np_utils.to_categorical(data_Y, num_class)\n",
    "    return data_X, data_Y, vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "reviews_Home_and_Kitchen_5.json:\n",
      "before:  551682\n",
      "after:  506623\n",
      "text shape: (20000, 100)\n",
      "vocabulary size: 59641\n",
      "x_train:  (16000, 100)\n",
      "y_train:  (16000, 2)\n",
      "x_test:  (4000, 100)\n",
      "y_test:  (4000, 2)\n",
      "vocabulary_size: 59641\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          17892300  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 300)          1442400   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 100, 300, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 98, 298, 100)      1000      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 49, 149, 100)      0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 49, 149, 100)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 730100)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1460202   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 20,795,902\n",
      "Trainable params: 20,795,902\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train...\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/5\n",
      "16000/16000 [==============================] - 2443s 153ms/step - loss: 0.5622 - acc: 0.6740 - val_loss: 0.3918 - val_acc: 0.8222\n",
      "Epoch 2/5\n",
      "16000/16000 [==============================] - 2292s 143ms/step - loss: 0.3507 - acc: 0.8501 - val_loss: 0.4151 - val_acc: 0.8160\n",
      "Epoch 3/5\n",
      "16000/16000 [==============================] - 2353s 147ms/step - loss: 0.2979 - acc: 0.8752 - val_loss: 0.3336 - val_acc: 0.8620\n",
      "Epoch 4/5\n",
      "16000/16000 [==============================] - 2367s 148ms/step - loss: 0.2626 - acc: 0.8935 - val_loss: 0.3325 - val_acc: 0.8687\n",
      "Epoch 5/5\n",
      "16000/16000 [==============================] - 2437s 152ms/step - loss: 0.2368 - acc: 0.9069 - val_loss: 0.3621 - val_acc: 0.8635\n",
      "4000/4000 [==============================] - 109s 27ms/step\n",
      "Test score: 0.36208009749650955\n",
      "Test accuracy: 0.8634999930113554\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.84      0.86      1994\n",
      "          1       0.85      0.89      0.87      2006\n",
      "\n",
      "avg / total       0.86      0.86      0.86      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 0\n",
    "num_class = 2\n",
    "for filename in filenames:\n",
    "    print(\"-----------------------------\")\n",
    "    print(filename + \":\")\n",
    "    X, Y, vocabulary_size = getXY(filename, num_class)\n",
    "    for train_index, test_index in randomShuffle.split(X):\n",
    "        X_train = X[train_index]\n",
    "        Y_train = Y[train_index, :]\n",
    "        X_test = X[test_index]\n",
    "        Y_test = Y[test_index, :]\n",
    "        \n",
    "        bidirectionalLSTMCNNModel(False, num_class, 'categorical_crossentropy', X_train, X_test, Y_train, Y_test, 10, 5, vocabulary_size, [], 'BLSTM+CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
