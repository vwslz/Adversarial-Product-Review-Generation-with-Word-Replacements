{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Reshape, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"reviews_Musical_Instruments_5.json\"]\n",
    "filename = \"reviews_Musical_Instruments_5.json\"\n",
    "t = Tokenizer()\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "\n",
    "dictionary_size = 0\n",
    "embed_dim = 300\n",
    "num_hidden_unit_lstm = 300\n",
    "num_filter = 100\n",
    "window_size = 3\n",
    "pool_size = 2\n",
    "batch_size = 10\n",
    "learning_rate = 0.1\n",
    "#AdaDelta\n",
    "dropout_embed = 0.5\n",
    "dropout_blstm = 0.2\n",
    "dropout_penultimate = 0.4\n",
    "l2_lambda = 0.000001\n",
    "\n",
    "randomShuffle = ShuffleSplit(n_splits=1, test_size=.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(filename, lines=True, orient=\"frame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>Not much to write about here, but it does exac...</td>\n",
       "      <td>02 28, 2014</td>\n",
       "      <td>A2IBPI20UZIR0U</td>\n",
       "      <td>cassandra tu \"Yeah, well, that's just like, u...</td>\n",
       "      <td>good</td>\n",
       "      <td>1393545600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[13, 14]</td>\n",
       "      <td>5</td>\n",
       "      <td>The product does exactly as it should and is q...</td>\n",
       "      <td>03 16, 2013</td>\n",
       "      <td>A14VAT5EAX3D9S</td>\n",
       "      <td>Jake</td>\n",
       "      <td>Jake</td>\n",
       "      <td>1363392000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5</td>\n",
       "      <td>The primary job of this device is to block the...</td>\n",
       "      <td>08 28, 2013</td>\n",
       "      <td>A195EZSQDW3E21</td>\n",
       "      <td>Rick Bennette \"Rick Bennette\"</td>\n",
       "      <td>It Does The Job Well</td>\n",
       "      <td>1377648000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>Nice windscreen protects my MXL mic and preven...</td>\n",
       "      <td>02 14, 2014</td>\n",
       "      <td>A2C00NNG1ZQQG2</td>\n",
       "      <td>RustyBill \"Sunday Rocker\"</td>\n",
       "      <td>GOOD WINDSCREEN FOR THE MONEY</td>\n",
       "      <td>1392336000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>This pop filter is great. It looks and perform...</td>\n",
       "      <td>02 21, 2014</td>\n",
       "      <td>A94QU4C90B1AX</td>\n",
       "      <td>SEAN MASLANKA</td>\n",
       "      <td>No more pops when I record my vocals.</td>\n",
       "      <td>1392940800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B00004Y2UT</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>So good that I bought another one.  Love the h...</td>\n",
       "      <td>12 21, 2012</td>\n",
       "      <td>A2A039TZMZHH9Y</td>\n",
       "      <td>Bill Lewey \"blewey\"</td>\n",
       "      <td>The Best Cable</td>\n",
       "      <td>1356048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B00004Y2UT</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>I have used monster cables for years, and with...</td>\n",
       "      <td>01 19, 2014</td>\n",
       "      <td>A1UPZM995ZAH90</td>\n",
       "      <td>Brian</td>\n",
       "      <td>Monster Standard 100 - 21' Instrument Cable</td>\n",
       "      <td>1390089600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B00004Y2UT</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>3</td>\n",
       "      <td>I now use this cable to run from the output of...</td>\n",
       "      <td>11 16, 2012</td>\n",
       "      <td>AJNFQI3YR6XJ5</td>\n",
       "      <td>Fender Guy \"Rick\"</td>\n",
       "      <td>Didn't fit my 1996 Fender Strat...</td>\n",
       "      <td>1353024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B00004Y2UT</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>Perfect for my Epiphone Sheraton II.  Monster ...</td>\n",
       "      <td>07 6, 2008</td>\n",
       "      <td>A3M1PLEYNDEYO8</td>\n",
       "      <td>G. Thomas \"Tom\"</td>\n",
       "      <td>Great cable</td>\n",
       "      <td>1215302400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B00004Y2UT</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>Monster makes the best cables and a lifetime w...</td>\n",
       "      <td>01 8, 2014</td>\n",
       "      <td>AMNTZU1YQN1TH</td>\n",
       "      <td>Kurt Robair</td>\n",
       "      <td>Best Instrument Cables On The Market</td>\n",
       "      <td>1389139200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin   helpful  overall  \\\n",
       "0  1384719342    [0, 0]        5   \n",
       "1  1384719342  [13, 14]        5   \n",
       "2  1384719342    [1, 1]        5   \n",
       "3  1384719342    [0, 0]        5   \n",
       "4  1384719342    [0, 0]        5   \n",
       "5  B00004Y2UT    [0, 0]        5   \n",
       "6  B00004Y2UT    [0, 0]        5   \n",
       "7  B00004Y2UT    [0, 0]        3   \n",
       "8  B00004Y2UT    [0, 0]        5   \n",
       "9  B00004Y2UT    [0, 0]        5   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  Not much to write about here, but it does exac...  02 28, 2014   \n",
       "1  The product does exactly as it should and is q...  03 16, 2013   \n",
       "2  The primary job of this device is to block the...  08 28, 2013   \n",
       "3  Nice windscreen protects my MXL mic and preven...  02 14, 2014   \n",
       "4  This pop filter is great. It looks and perform...  02 21, 2014   \n",
       "5  So good that I bought another one.  Love the h...  12 21, 2012   \n",
       "6  I have used monster cables for years, and with...  01 19, 2014   \n",
       "7  I now use this cable to run from the output of...  11 16, 2012   \n",
       "8  Perfect for my Epiphone Sheraton II.  Monster ...   07 6, 2008   \n",
       "9  Monster makes the best cables and a lifetime w...   01 8, 2014   \n",
       "\n",
       "       reviewerID                                      reviewerName  \\\n",
       "0  A2IBPI20UZIR0U  cassandra tu \"Yeah, well, that's just like, u...   \n",
       "1  A14VAT5EAX3D9S                                              Jake   \n",
       "2  A195EZSQDW3E21                     Rick Bennette \"Rick Bennette\"   \n",
       "3  A2C00NNG1ZQQG2                         RustyBill \"Sunday Rocker\"   \n",
       "4   A94QU4C90B1AX                                     SEAN MASLANKA   \n",
       "5  A2A039TZMZHH9Y                               Bill Lewey \"blewey\"   \n",
       "6  A1UPZM995ZAH90                                             Brian   \n",
       "7   AJNFQI3YR6XJ5                                 Fender Guy \"Rick\"   \n",
       "8  A3M1PLEYNDEYO8                                   G. Thomas \"Tom\"   \n",
       "9   AMNTZU1YQN1TH                                       Kurt Robair   \n",
       "\n",
       "                                       summary  unixReviewTime  \n",
       "0                                         good      1393545600  \n",
       "1                                         Jake      1363392000  \n",
       "2                         It Does The Job Well      1377648000  \n",
       "3                GOOD WINDSCREEN FOR THE MONEY      1392336000  \n",
       "4        No more pops when I record my vocals.      1392940800  \n",
       "5                               The Best Cable      1356048000  \n",
       "6  Monster Standard 100 - 21' Instrument Cable      1390089600  \n",
       "7           Didn't fit my 1996 Fender Strat...      1353024000  \n",
       "8                                  Great cable      1215302400  \n",
       "9         Best Instrument Cables On The Market      1389139200  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['overall'] = np.where(data['overall'] < 3, 0, data.overall)\n",
    "data['overall'] = np.where(data['overall'] == 3, 1, data.overall)\n",
    "data['overall'] = np.where(data['overall'] > 3, 2, data.overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>2</td>\n",
       "      <td>Not much to write about here, but it does exac...</td>\n",
       "      <td>02 28, 2014</td>\n",
       "      <td>A2IBPI20UZIR0U</td>\n",
       "      <td>cassandra tu \"Yeah, well, that's just like, u...</td>\n",
       "      <td>good</td>\n",
       "      <td>1393545600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[13, 14]</td>\n",
       "      <td>2</td>\n",
       "      <td>The product does exactly as it should and is q...</td>\n",
       "      <td>03 16, 2013</td>\n",
       "      <td>A14VAT5EAX3D9S</td>\n",
       "      <td>Jake</td>\n",
       "      <td>Jake</td>\n",
       "      <td>1363392000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>2</td>\n",
       "      <td>The primary job of this device is to block the...</td>\n",
       "      <td>08 28, 2013</td>\n",
       "      <td>A195EZSQDW3E21</td>\n",
       "      <td>Rick Bennette \"Rick Bennette\"</td>\n",
       "      <td>It Does The Job Well</td>\n",
       "      <td>1377648000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>2</td>\n",
       "      <td>Nice windscreen protects my MXL mic and preven...</td>\n",
       "      <td>02 14, 2014</td>\n",
       "      <td>A2C00NNG1ZQQG2</td>\n",
       "      <td>RustyBill \"Sunday Rocker\"</td>\n",
       "      <td>GOOD WINDSCREEN FOR THE MONEY</td>\n",
       "      <td>1392336000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>2</td>\n",
       "      <td>This pop filter is great. It looks and perform...</td>\n",
       "      <td>02 21, 2014</td>\n",
       "      <td>A94QU4C90B1AX</td>\n",
       "      <td>SEAN MASLANKA</td>\n",
       "      <td>No more pops when I record my vocals.</td>\n",
       "      <td>1392940800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B00004Y2UT</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>2</td>\n",
       "      <td>So good that I bought another one.  Love the h...</td>\n",
       "      <td>12 21, 2012</td>\n",
       "      <td>A2A039TZMZHH9Y</td>\n",
       "      <td>Bill Lewey \"blewey\"</td>\n",
       "      <td>The Best Cable</td>\n",
       "      <td>1356048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B00004Y2UT</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>2</td>\n",
       "      <td>I have used monster cables for years, and with...</td>\n",
       "      <td>01 19, 2014</td>\n",
       "      <td>A1UPZM995ZAH90</td>\n",
       "      <td>Brian</td>\n",
       "      <td>Monster Standard 100 - 21' Instrument Cable</td>\n",
       "      <td>1390089600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B00004Y2UT</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>I now use this cable to run from the output of...</td>\n",
       "      <td>11 16, 2012</td>\n",
       "      <td>AJNFQI3YR6XJ5</td>\n",
       "      <td>Fender Guy \"Rick\"</td>\n",
       "      <td>Didn't fit my 1996 Fender Strat...</td>\n",
       "      <td>1353024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B00004Y2UT</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>2</td>\n",
       "      <td>Perfect for my Epiphone Sheraton II.  Monster ...</td>\n",
       "      <td>07 6, 2008</td>\n",
       "      <td>A3M1PLEYNDEYO8</td>\n",
       "      <td>G. Thomas \"Tom\"</td>\n",
       "      <td>Great cable</td>\n",
       "      <td>1215302400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B00004Y2UT</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>2</td>\n",
       "      <td>Monster makes the best cables and a lifetime w...</td>\n",
       "      <td>01 8, 2014</td>\n",
       "      <td>AMNTZU1YQN1TH</td>\n",
       "      <td>Kurt Robair</td>\n",
       "      <td>Best Instrument Cables On The Market</td>\n",
       "      <td>1389139200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin   helpful  overall  \\\n",
       "0  1384719342    [0, 0]        2   \n",
       "1  1384719342  [13, 14]        2   \n",
       "2  1384719342    [1, 1]        2   \n",
       "3  1384719342    [0, 0]        2   \n",
       "4  1384719342    [0, 0]        2   \n",
       "5  B00004Y2UT    [0, 0]        2   \n",
       "6  B00004Y2UT    [0, 0]        2   \n",
       "7  B00004Y2UT    [0, 0]        1   \n",
       "8  B00004Y2UT    [0, 0]        2   \n",
       "9  B00004Y2UT    [0, 0]        2   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  Not much to write about here, but it does exac...  02 28, 2014   \n",
       "1  The product does exactly as it should and is q...  03 16, 2013   \n",
       "2  The primary job of this device is to block the...  08 28, 2013   \n",
       "3  Nice windscreen protects my MXL mic and preven...  02 14, 2014   \n",
       "4  This pop filter is great. It looks and perform...  02 21, 2014   \n",
       "5  So good that I bought another one.  Love the h...  12 21, 2012   \n",
       "6  I have used monster cables for years, and with...  01 19, 2014   \n",
       "7  I now use this cable to run from the output of...  11 16, 2012   \n",
       "8  Perfect for my Epiphone Sheraton II.  Monster ...   07 6, 2008   \n",
       "9  Monster makes the best cables and a lifetime w...   01 8, 2014   \n",
       "\n",
       "       reviewerID                                      reviewerName  \\\n",
       "0  A2IBPI20UZIR0U  cassandra tu \"Yeah, well, that's just like, u...   \n",
       "1  A14VAT5EAX3D9S                                              Jake   \n",
       "2  A195EZSQDW3E21                     Rick Bennette \"Rick Bennette\"   \n",
       "3  A2C00NNG1ZQQG2                         RustyBill \"Sunday Rocker\"   \n",
       "4   A94QU4C90B1AX                                     SEAN MASLANKA   \n",
       "5  A2A039TZMZHH9Y                               Bill Lewey \"blewey\"   \n",
       "6  A1UPZM995ZAH90                                             Brian   \n",
       "7   AJNFQI3YR6XJ5                                 Fender Guy \"Rick\"   \n",
       "8  A3M1PLEYNDEYO8                                   G. Thomas \"Tom\"   \n",
       "9   AMNTZU1YQN1TH                                       Kurt Robair   \n",
       "\n",
       "                                       summary  unixReviewTime  \n",
       "0                                         good      1393545600  \n",
       "1                                         Jake      1363392000  \n",
       "2                         It Does The Job Well      1377648000  \n",
       "3                GOOD WINDSCREEN FOR THE MONEY      1392336000  \n",
       "4        No more pops when I record my vocals.      1392940800  \n",
       "5                               The Best Cable      1356048000  \n",
       "6  Monster Standard 100 - 21' Instrument Cable      1390089600  \n",
       "7           Didn't fit my 1996 Fender Strat...      1353024000  \n",
       "8                                  Great cable      1215302400  \n",
       "9         Best Instrument Cables On The Market      1389139200  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 467\t\t4.55%\n",
      "1: 772\t\t7.52%\n",
      "2: 9022\t\t87.93%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label = list(data.iloc[:, 2].values)\n",
    "for j in range(3):\n",
    "    print(str(j) + \": \" + str(label.count(j)) + \"\\t\\t\" + str(round(label.count(j) / len(label) * 100, 2)) + \"%\")\n",
    "\n",
    "plt.plot([0, 1, 2], [label.count(0), label.count(1), label.count(2)], 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingMatrix(vocabulary_size):\n",
    "    embeddings_index = dict()\n",
    "    f = open('glove.6b/glove.6B.300d.txt', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    embedding_matrix = np.zeros((vocabulary_size, embed_dim))\n",
    "    for word, i in t.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXY(filename):\n",
    "    data = pd.read_json(filename, lines=True, orient=\"frame\")\n",
    "    data['overall'] = np.where(data['overall'] < 3, 0, data.overall)\n",
    "    data['overall'] = np.where(data['overall'] == 3, 1, data.overall)\n",
    "    data['overall'] = np.where(data['overall'] > 3, 2, data.overall)\n",
    "    \n",
    "    # down sample\n",
    "    data_0 = data.loc[data['overall'] == 0]\n",
    "    data_1 = data.loc[data['overall'] == 1]\n",
    "    data_2 = data.loc[data['overall'] == 2].sample(len(data_0))\n",
    "    \n",
    "    data = data_0.append(data_1).append(data_2).sample(frac=1)\n",
    "    \n",
    "    good_columns = [3]\n",
    "    num_rating = 3\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    data_X = data.iloc[:, good_columns]\n",
    "    data_Y = data.iloc[:, 2]\n",
    "    \n",
    "    reviewTextList = data.reviewText.values\n",
    "    t.fit_on_texts(reviewTextList)\n",
    "    text = t.texts_to_sequences(reviewTextList)\n",
    "    text = sequence.pad_sequences(text, maxlen=maxlen)\n",
    "    print('text shape:', text.shape)\n",
    "    vocabulary_size = len(t.word_index) + 1\n",
    "    print('vocabulary size:', vocabulary_size)\n",
    "    \n",
    "    data_X = np.array(text)\n",
    "    data_Y = np_utils.to_categorical(data_Y, num_rating)\n",
    "    return data_X, data_Y, vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectionalLSTMModel(x_train, x_test, y_train, y_test, batch_size, epochs):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "    model.add(Bidirectional(LSTM(num_hidden_unit_lstm)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "    print(model.summary())\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='Adadelta',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print('Train...')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=[x_test, y_test])\n",
    "    \n",
    "    score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    print('Test score:', score)\n",
    "    print('Test accuracy:', acc)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    y_pred = model.predict_classes(x_test)\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectionalLSTMCNNModel(x_train, x_test, y_train, y_test, batch_size, epochs, vocabulary_size, embedding_matrix):\n",
    "    print(\"x_train: \", x_train.shape)\n",
    "    print(\"y_train: \", y_train.shape)\n",
    "    print(\"x_test: \", x_test.shape)\n",
    "    print(\"y_test: \", y_test.shape)\n",
    "#     print(\"x_train:\",x_train[0])\n",
    "#     print(\"x_test:\",x_test[0])\n",
    "#     print('y_train:',y_train[0])\n",
    "#     print('y_test',y_test[0])\n",
    "    model = Sequential()\n",
    "    print(\"vocabulary_size:\", vocabulary_size)\n",
    "#     model.add(Embedding(input_dim=vocabulary_size, output_dim=embed_dim, input_length=maxlen))\n",
    "    model.add(Embedding(input_dim=vocabulary_size, output_dim=embed_dim, input_length=maxlen, weights=[embedding_matrix], trainable=False))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            LSTM(\n",
    "                num_hidden_unit_lstm, \n",
    "                return_sequences=True, \n",
    "                 kernel_regularizer=l2(l2_lambda)\n",
    "            ), \n",
    "            merge_mode=\"sum\"\n",
    "        ))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Reshape((maxlen, embed_dim, 1)))\n",
    "    model.add(Conv2D(num_filter,\n",
    "                     kernel_size=(3,3),\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1,\n",
    "                      kernel_regularizer=l2(l2_lambda)\n",
    "                    ))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(3, kernel_regularizer=l2(l2_lambda)))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    print(model.summary())\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer='Adadelta',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print('Train...')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=[x_test, y_test])\n",
    "    \n",
    "    score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    print('Test score:', score)\n",
    "    print('Test accuracy:', acc)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    y_pred = model.predict_classes(x_test)\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text shape: (2478, 100)\n",
      "vocabulary size: 11310\n",
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 0\n",
    "filename = filenames[0]\n",
    "X, Y, vocabulary_size = getXY(filename)\n",
    "embedding_matrix = getEmbeddingMatrix(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "reviews_Musical_Instruments_5.json:\n",
      "text shape: (2478, 100)\n",
      "vocabulary size: 13327\n",
      "Loaded 400000 word vectors.\n",
      "x_train:  (1982, 100)\n",
      "y_train:  (1982, 3)\n",
      "x_test:  (496, 100)\n",
      "y_test:  (496, 3)\n",
      "vocabulary_size: 13327\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          3998100   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 300)          1442400   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 100, 300, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 98, 298, 100)      1000      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 49, 149, 100)      0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 49, 149, 100)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 730100)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 2190303   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 7,631,803\n",
      "Trainable params: 3,633,703\n",
      "Non-trainable params: 3,998,100\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train...\n",
      "Train on 1982 samples, validate on 496 samples\n",
      "Epoch 1/30\n",
      "1982/1982 [==============================] - 93s 47ms/step - loss: 1.0555 - acc: 0.4980 - val_loss: 1.1461 - val_acc: 0.4879\n",
      "Epoch 2/30\n",
      "1982/1982 [==============================] - 84s 43ms/step - loss: 0.9780 - acc: 0.5136 - val_loss: 1.0571 - val_acc: 0.5121\n",
      "Epoch 3/30\n",
      "1982/1982 [==============================] - 85s 43ms/step - loss: 0.9103 - acc: 0.5590 - val_loss: 0.9966 - val_acc: 0.5081\n",
      "Epoch 4/30\n",
      "1982/1982 [==============================] - 86s 44ms/step - loss: 0.8350 - acc: 0.6251 - val_loss: 0.9844 - val_acc: 0.5343\n",
      "Epoch 5/30\n",
      "1982/1982 [==============================] - 85s 43ms/step - loss: 0.7464 - acc: 0.6756 - val_loss: 0.9562 - val_acc: 0.5444\n",
      "Epoch 6/30\n",
      "1982/1982 [==============================] - 84s 42ms/step - loss: 0.5749 - acc: 0.7558 - val_loss: 1.0801 - val_acc: 0.5323\n",
      "Epoch 7/30\n",
      "1982/1982 [==============================] - 84s 42ms/step - loss: 0.4582 - acc: 0.8239 - val_loss: 0.9712 - val_acc: 0.5444\n",
      "Epoch 8/30\n",
      "1982/1982 [==============================] - 84s 42ms/step - loss: 0.2986 - acc: 0.8814 - val_loss: 1.1795 - val_acc: 0.5282\n",
      "Epoch 9/30\n",
      "1982/1982 [==============================] - 83s 42ms/step - loss: 0.2352 - acc: 0.9168 - val_loss: 1.3166 - val_acc: 0.5524\n",
      "Epoch 10/30\n",
      "1982/1982 [==============================] - 83s 42ms/step - loss: 0.1726 - acc: 0.9379 - val_loss: 1.5439 - val_acc: 0.4859\n",
      "Epoch 11/30\n",
      "1982/1982 [==============================] - 84s 42ms/step - loss: 0.1467 - acc: 0.9445 - val_loss: 1.3240 - val_acc: 0.5585\n",
      "Epoch 12/30\n",
      "1982/1982 [==============================] - 84s 42ms/step - loss: 0.1115 - acc: 0.9657 - val_loss: 1.3360 - val_acc: 0.5504\n",
      "Epoch 13/30\n",
      "1982/1982 [==============================] - 83s 42ms/step - loss: 0.0892 - acc: 0.9723 - val_loss: 1.4929 - val_acc: 0.5565\n",
      "Epoch 14/30\n",
      "1982/1982 [==============================] - 83s 42ms/step - loss: 0.0652 - acc: 0.9798 - val_loss: 1.6267 - val_acc: 0.5363\n",
      "Epoch 15/30\n",
      "1982/1982 [==============================] - 83s 42ms/step - loss: 0.0578 - acc: 0.9844 - val_loss: 1.7702 - val_acc: 0.5645\n",
      "Epoch 16/30\n",
      "1982/1982 [==============================] - 84s 42ms/step - loss: 0.0605 - acc: 0.9808 - val_loss: 2.0620 - val_acc: 0.4677\n",
      "Epoch 17/30\n",
      "1982/1982 [==============================] - 83s 42ms/step - loss: 0.0650 - acc: 0.9808 - val_loss: 1.5629 - val_acc: 0.5343\n",
      "Epoch 18/30\n",
      "1982/1982 [==============================] - 83s 42ms/step - loss: 0.0389 - acc: 0.9909 - val_loss: 1.8327 - val_acc: 0.5222\n",
      "Epoch 19/30\n",
      "1982/1982 [==============================] - 83s 42ms/step - loss: 0.0516 - acc: 0.9884 - val_loss: 1.8064 - val_acc: 0.5464\n",
      "Epoch 20/30\n",
      "1982/1982 [==============================] - 84s 42ms/step - loss: 0.0371 - acc: 0.9919 - val_loss: 1.8006 - val_acc: 0.5282\n",
      "Epoch 21/30\n",
      "1982/1982 [==============================] - 87s 44ms/step - loss: 0.0365 - acc: 0.9874 - val_loss: 1.8581 - val_acc: 0.5403\n",
      "Epoch 22/30\n",
      "1982/1982 [==============================] - 84s 43ms/step - loss: 0.0375 - acc: 0.9919 - val_loss: 1.9364 - val_acc: 0.5444\n",
      "Epoch 23/30\n",
      "1982/1982 [==============================] - 85s 43ms/step - loss: 0.0376 - acc: 0.9914 - val_loss: 1.8409 - val_acc: 0.5625\n",
      "Epoch 24/30\n",
      "1982/1982 [==============================] - 84s 42ms/step - loss: 0.0360 - acc: 0.9919 - val_loss: 1.7933 - val_acc: 0.5726\n",
      "Epoch 25/30\n",
      "1982/1982 [==============================] - 86s 43ms/step - loss: 0.0354 - acc: 0.9909 - val_loss: 1.7929 - val_acc: 0.5423\n",
      "Epoch 26/30\n",
      "1982/1982 [==============================] - 86s 43ms/step - loss: 0.0345 - acc: 0.9904 - val_loss: 1.8868 - val_acc: 0.5544\n",
      "Epoch 27/30\n",
      "1982/1982 [==============================] - 84s 42ms/step - loss: 0.0279 - acc: 0.9939 - val_loss: 2.0243 - val_acc: 0.5423\n",
      "Epoch 28/30\n",
      "1982/1982 [==============================] - 83s 42ms/step - loss: 0.0164 - acc: 0.9950 - val_loss: 2.1098 - val_acc: 0.5544\n",
      "Epoch 29/30\n",
      "1982/1982 [==============================] - 85s 43ms/step - loss: 0.0251 - acc: 0.9934 - val_loss: 2.1536 - val_acc: 0.5383\n",
      "Epoch 30/30\n",
      "1982/1982 [==============================] - 86s 44ms/step - loss: 0.0323 - acc: 0.9929 - val_loss: 1.9846 - val_acc: 0.5464\n",
      "496/496 [==============================] - 5s 10ms/step\n",
      "Test score: 1.9845556592148157\n",
      "Test accuracy: 0.5463709706861165\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.34      0.28      0.31        92\n",
      "          1       0.46      0.38      0.42       162\n",
      "          2       0.64      0.76      0.69       242\n",
      "\n",
      "avg / total       0.53      0.55      0.53       496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 0\n",
    "for filename in filenames:\n",
    "    print(\"-----------------------------\")\n",
    "    print(filename + \":\")\n",
    "    X, Y, vocabulary_size = getXY(filename)\n",
    "    embedding_matrix = getEmbeddingMatrix(vocabulary_size)\n",
    "    for train_index, test_index in randomShuffle.split(X):\n",
    "        X_train = X[train_index]\n",
    "        Y_train = Y[train_index, :]\n",
    "        X_test = X[test_index]\n",
    "        Y_test = Y[test_index, :]\n",
    "        \n",
    "        bidirectionalLSTMCNNModel(X_train, X_test, Y_train, Y_test, 10, 30, vocabulary_size, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 classes considering as polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXY(filename, num_class):\n",
    "    data = pd.read_json(filename, lines=True, orient=\"frame\")\n",
    "    data['overall'] = np.where(data['overall'] < 3, 0, data.overall)\n",
    "    data['overall'] = np.where(data['overall'] > 3, 1, data.overall)\n",
    "    \n",
    "    # down sample\n",
    "    data_0 = data.loc[data['overall'] == 0]\n",
    "    data_1 = data.loc[data['overall'] == 1].sample(len(data_0))\n",
    "    \n",
    "    data = data_0.append(data_1).sample(frac=1)\n",
    "    \n",
    "    good_columns = [3]\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    data_X = data.iloc[:, good_columns]\n",
    "    data_Y = data.iloc[:, 2]\n",
    "    \n",
    "    reviewTextList = data.reviewText.values\n",
    "    t.fit_on_texts(reviewTextList)\n",
    "    text = t.texts_to_sequences(reviewTextList)\n",
    "    text = sequence.pad_sequences(text, maxlen=maxlen)\n",
    "    print('text shape:', text.shape)\n",
    "    vocabulary_size = len(t.word_index) + 1\n",
    "    print('vocabulary size:', vocabulary_size)\n",
    "    \n",
    "    data_X = np.array(text)\n",
    "    data_Y = np_utils.to_categorical(data_Y, num_class)\n",
    "    return data_X, data_Y, vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectionalLSTMCNNModel(containsGlove, num_class, str_loss, x_train, x_test, y_train, y_test, batch_size, epochs, vocabulary_size, embedding_matrix):\n",
    "    print(\"x_train: \", x_train.shape)\n",
    "    print(\"y_train: \", y_train.shape)\n",
    "    print(\"x_test: \", x_test.shape)\n",
    "    print(\"y_test: \", y_test.shape)\n",
    "#     print(\"x_train:\",x_train[0])\n",
    "#     print(\"x_test:\",x_test[0])\n",
    "#     print('y_train:',y_train[0])\n",
    "#     print('y_test',y_test[0])\n",
    "    model = Sequential()\n",
    "    print(\"vocabulary_size:\", vocabulary_size)\n",
    "    if (containsGlove == False):\n",
    "        model.add(Embedding(input_dim=vocabulary_size, output_dim=embed_dim, input_length=maxlen))\n",
    "    else:\n",
    "        model.add(Embedding(input_dim=vocabulary_size, output_dim=embed_dim, input_length=maxlen, weights=[embedding_matrix], trainable=False))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            LSTM(\n",
    "                num_hidden_unit_lstm, \n",
    "                return_sequences=True, \n",
    "                 kernel_regularizer=l2(l2_lambda)\n",
    "            ), \n",
    "            merge_mode=\"sum\"\n",
    "        ))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Reshape((maxlen, embed_dim, 1)))\n",
    "    model.add(Conv2D(num_filter,\n",
    "                     kernel_size=(3,3),\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1,\n",
    "                      kernel_regularizer=l2(l2_lambda)\n",
    "                    ))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_class, kernel_regularizer=l2(l2_lambda)))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    print(model.summary())\n",
    "    model.compile(loss=str_loss, \n",
    "                  optimizer='Adadelta',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print('Train...')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=[x_test, y_test])\n",
    "    \n",
    "    score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    print('Test score:', score)\n",
    "    print('Test accuracy:', acc)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    y_pred = model.predict_classes(x_test)\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"reviews_Home_and_Kitchen_5.json\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "reviews_Home_and_Kitchen_5.json:\n",
      "text shape: (102838, 100)\n",
      "vocabulary size: 75011\n",
      "Loaded 400000 word vectors.\n",
      "x_train:  (82270, 100)\n",
      "y_train:  (82270, 2)\n",
      "x_test:  (20568, 100)\n",
      "y_test:  (20568, 2)\n",
      "vocabulary_size: 75011\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          22503300  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 300)          1442400   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 100, 300, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 98, 298, 100)      1000      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 49, 149, 100)      0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 49, 149, 100)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 730100)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1460202   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 25,406,902\n",
      "Trainable params: 25,406,902\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train...\n",
      "Train on 82270 samples, validate on 20568 samples\n",
      "Epoch 1/20\n",
      " 9690/82270 [==>...........................] - ETA: 1:21:29 - loss: 0.5919 - acc: 0.6636"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 0\n",
    "num_class = 2\n",
    "for filename in filenames:\n",
    "    print(\"-----------------------------\")\n",
    "    print(filename + \":\")\n",
    "    X, Y, vocabulary_size = getXY(filename, num_class)\n",
    "    embedding_matrix = getEmbeddingMatrix(vocabulary_size)\n",
    "    for train_index, test_index in randomShuffle.split(X):\n",
    "        X_train = X[train_index]\n",
    "        Y_train = Y[train_index, :]\n",
    "        X_test = X[test_index]\n",
    "        Y_test = Y[test_index, :]\n",
    "        \n",
    "        bidirectionalLSTMCNNModel(False, num_class, 'binary_crossentropy', X_train, X_test, Y_train, Y_test, 10, 20, vocabulary_size, embedding_matrix)\n",
    "        bidirectionalLSTMCNNModel(True, num_class, 'binary_crossentropy', X_train, X_test, Y_train, Y_test, 10, 20, vocabulary_size, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
