{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk.data\n",
    "import collections\n",
    "import random\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames = [\"data/reviews_Home_and_Kitchen_5.json\"]\n",
    "# filename = \"data/reviews_Home_and_Kitchen_5.json\"\n",
    "file_name = \"rawdata_100.pkl\"\n",
    "file_name_s = \"rawdata_20.pkl\"\n",
    "filename_adversarial_wordnet = \"data_adversial_wordnet\"\n",
    "filename_extension = \".pkl\"\n",
    "filename_adversarial_glove = \"data_adversial_glove.pkl\"\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "batch_size = 10\n",
    "num_class = 2\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_TO_WORDNET = {\n",
    "    'NN': wn.NOUN,\n",
    "    'JJ': wn.ADJ,\n",
    "    'JJR': wn.ADJ,\n",
    "    'JJS': wn.ADJ,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addToReplacement(token, synDict):\n",
    "    if token[1] not in POS_TO_WORDNET: return None\n",
    "    w = token[0].lower()\n",
    "    wn_pos = POS_TO_WORDNET[token[1]]\n",
    "    synsets = wn.synsets(w, wn_pos)\n",
    "    if not synsets: return None\n",
    "    synset = synsets[0]\n",
    "    synonyms = []\n",
    "    antonyms = []\n",
    "\n",
    "    for syn in wn.synsets(w, wn_pos):\n",
    "        for lem in syn.lemmas():\n",
    "            if (lem.name() != w):\n",
    "#                 synonyms.append(lem.name())\n",
    "                synDict[w] = lem.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def makeAdversarialDataset(numReplacement, numSample, texts, data):\n",
    "    set_adversial = []\n",
    "    set_unchanged = []\n",
    "    index = 0\n",
    "    sampleOutput = []\n",
    "    for i, row in data.iterrows():\n",
    "        synsetDictionary = {}\n",
    "\n",
    "        text = texts[i]\n",
    "        tokenized = tokenizer.tokenize(text)\n",
    "        words = word_tokenize(text)\n",
    "        tokens = nltk.pos_tag(words)\n",
    "        for token in tokens:\n",
    "            addToReplacement(token, synsetDictionary)\n",
    "        if len(synsetDictionary) > 0:\n",
    "            output = text\n",
    "            for key in random.sample(synsetDictionary.keys(), min(numReplacement, len(synsetDictionary))):\n",
    "                output = output.replace(key, synsetDictionary[key])\n",
    "                if (numSample > 0):\n",
    "                    sampleOutput.append(\"[\" + str(i) + \"]: key-\" + str(key) + \" value-\" + str(synsetDictionary[key]))\n",
    "                del synsetDictionary[key]\n",
    "            data.at[i,'reviewText'] = output\n",
    "            numSample = numSample - 1\n",
    "            set_adversial.append(i)\n",
    "        else:\n",
    "            set_unchanged.append(i)\n",
    "        print(\"progress bar: \", index, \"/\", len(data))\n",
    "        clear_output(wait=True)\n",
    "        index = index + 1\n",
    "\n",
    "    print(sampleOutput)\n",
    "    return set_adversial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Regular dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[383985]: key-big value-with_child', '[383985]: key-everyday value-daily', '[383985]: key-tile value-roofing_tile', '[204506]: key-great value-with_child', '[204506]: key-happy value-well-chosen', '[204506]: key-door value-threshold', '[4025]: key-chance value-prospect', '[4025]: key-i value-ane', '[4025]: key-terrible value-tremendous']\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"Numer of max replacement is \", i + 1, \": \")\n",
    "    data = pd.read_pickle(file_name)\n",
    "    texts = data['reviewText']\n",
    "    set_adversial = makeAdversarialDataset(i + 1, 3, texts, data)\n",
    "    data = data.loc[set_adversial,:]\n",
    "    data.to_pickle(filename_adversarial_wordnet + \"_\" + str(i + 1) + filename_extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### replace named entity and numbers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[383985]: key-everyday value-daily', '[383985]: key-big value-with_child', '[383985]: key-dirt value-malicious_gossip', '[383985]: key-tile value-roofing_tile', '[383985]: key-vacuum value-vacuum_cleaner', '[383985]: key-product value-Cartesian_product', '[383985]: key-carpet value-carpeting', '[204506]: key-great value-with_child', '[204506]: key-happy value-well-chosen', '[204506]: key-door value-threshold', '[204506]: key-storage value-warehousing', '[204506]: key-purchase value-leverage', '[204506]: key-product value-Cartesian_product', '[4025]: key-sharp value-precipitous', '[4025]: key-frustration value-foiling', '[4025]: key-bad value-defective', '[4025]: key-chance value-prospect', '[4025]: key-i value-ane', '[4025]: key-terrible value-tremendous']\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle(file_name)\n",
    "texts = data['reviewText']\n",
    "set_adversial = makeAdversarialDataset(100000, 3, texts, data)\n",
    "data = data.loc[set_adversial,:]\n",
    "data.to_pickle(filename_adversarial_wordnet + filename_extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### smaller set of 20568 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[268965]: key-only value-alone', '[268965]: key-good value-unspoilt', '[268965]: key-night value-Night', '[268965]: key-cup value-loving_cup', '[268965]: key-timer value-timekeeper', '[268965]: key-issue value-publication', '[268965]: key-rest value-quietus', '[268965]: key-make value-shuffling', '[268965]: key-snob value-snoot', '[268965]: key-morning value-dawn', '[268965]: key-stuff value-poppycock', '[268965]: key-maker value-manufacturing_business', '[268965]: key-ground value-undercoat', '[268965]: key-i value-I', '[268965]: key-coffee value-burnt_umber', '[268965]: key-alarm value-alarm_clock', '[268965]: key-husband value-married_man', '[172151]: key-review value-inspection', '[172151]: key-fan value-lover', '[172151]: key-water value-weewee', '[172151]: key-defective value-bad', '[172151]: key-product value-Cartesian_product', '[172151]: key-i value-I', '[172151]: key-resistant value-repellent', '[172151]: key-own value-ain', '[172151]: key-possible value-potential', '[172151]: key-fine value-amercement', '[172151]: key-excellent value-splendid', \"[172151]: key-company value-ship's_company\", '[172151]: key-unit value-whole', '[172151]: key-reason value-grounds', '[172151]: key-sure value-indisputable', '[172151]: key-month value-calendar_month', '[295601]: key-i value-I', '[295601]: key-pan value-genus_Pan', '[295601]: key-best value-good', '[295601]: key-cooking value-preparation', '[295601]: key-great value-with_child', '[295601]: key-price value-Mary_Leontyne_Price']\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle(file_name_s)\n",
    "texts = data['reviewText']\n",
    "set_adversial = makeAdversarialDataset(100000, 3, texts, data)\n",
    "data = data.loc[set_adversial,:]\n",
    "data.to_pickle(filename_adversarial_wordnet + \"_s\" + filename_extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[268965]: key-snob value-snoot', '[268965]: key-issue value-publication', '[268965]: key-good value-unspoilt', '[172151]: key-reason value-grounds', '[172151]: key-resistant value-repellent', '[172151]: key-month value-calendar_month', '[295601]: key-pan value-genus_Pan', '[295601]: key-price value-Mary_Leontyne_Price', '[295601]: key-i value-I']\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"Numer of max replacement is \", i + 1, \": \")\n",
    "    data = pd.read_pickle(file_name_s)\n",
    "    texts = data['reviewText']\n",
    "    set_adversial = makeAdversarialDataset(i + 1, 3, texts, data)\n",
    "    data = data.loc[set_adversial,:]\n",
    "    data.to_pickle(filename_adversarial_wordnet + \"_\" + str(i + 1) + \"_s\" + filename_extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate Adversarial samples for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ori = pd.read_pickle(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(file_name)\n",
    "texts = data['reviewText']\n",
    "set_adversial = makeAdversarialDataset(100000, 3, texts, data)\n",
    "data = data.loc[set_adversial,:]\n",
    "data.to_pickle(filename_adversarial_wordnet + filename_extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>489257</th>\n",
       "      <td>1</td>\n",
       "      <td>I use this set for both general food storage i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318868</th>\n",
       "      <td>0</td>\n",
       "      <td>Not a huge fan . I like to have a side for dam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527578</th>\n",
       "      <td>1</td>\n",
       "      <td>I have always wanted to have a French press at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102291</th>\n",
       "      <td>0</td>\n",
       "      <td>I purchased this to replace a breadmaker that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137178</th>\n",
       "      <td>0</td>\n",
       "      <td>I bought this pan because my 12 '' pan needed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233040</th>\n",
       "      <td>0</td>\n",
       "      <td>Just sayin ' . I mean , honestly . Do we reall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413128</th>\n",
       "      <td>1</td>\n",
       "      <td>This rack fits the sink perfectly , and is n't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149988</th>\n",
       "      <td>0</td>\n",
       "      <td>I bought one of the 20X72 inch mats for my hus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18029</th>\n",
       "      <td>1</td>\n",
       "      <td>I have four different sizes of these scoops an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129849</th>\n",
       "      <td>1</td>\n",
       "      <td>These replacement rubbers are perfect for use ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall                                         reviewText\n",
       "489257        1  I use this set for both general food storage i...\n",
       "318868        0  Not a huge fan . I like to have a side for dam...\n",
       "527578        1  I have always wanted to have a French press at...\n",
       "102291        0  I purchased this to replace a breadmaker that ...\n",
       "137178        0  I bought this pan because my 12 '' pan needed ...\n",
       "233040        0  Just sayin ' . I mean , honestly . Do we reall...\n",
       "413128        1  This rack fits the sink perfectly , and is n't...\n",
       "149988        0  I bought one of the 20X72 inch mats for my hus...\n",
       "18029         1  I have four different sizes of these scoops an...\n",
       "129849        1  These replacement rubbers are perfect for use ..."
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(adversarial_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>489257</th>\n",
       "      <td>1</td>\n",
       "      <td>I use this set for both general food storage i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318868</th>\n",
       "      <td>0</td>\n",
       "      <td>Not a huge fan . I like to have a side for dam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527578</th>\n",
       "      <td>1</td>\n",
       "      <td>I have always wanted to have a French press at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102291</th>\n",
       "      <td>0</td>\n",
       "      <td>I purchased this to replace a breadmaker that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137178</th>\n",
       "      <td>0</td>\n",
       "      <td>I bought this pan because my 12 '' pan needed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233040</th>\n",
       "      <td>0</td>\n",
       "      <td>Just sayin ' . I mean , honestly . Do we reall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413128</th>\n",
       "      <td>1</td>\n",
       "      <td>This rack fits the sink perfectly , and is n't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149988</th>\n",
       "      <td>0</td>\n",
       "      <td>I bought one of the 20X72 inch mats for my hus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18029</th>\n",
       "      <td>1</td>\n",
       "      <td>I have four different sizes of these scoops an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129849</th>\n",
       "      <td>1</td>\n",
       "      <td>These replacement rubbers are perfect for use ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall                                         reviewText\n",
       "489257        1  I use this set for both general food storage i...\n",
       "318868        0  Not a huge fan . I like to have a side for dam...\n",
       "527578        1  I have always wanted to have a French press at...\n",
       "102291        0  I purchased this to replace a breadmaker that ...\n",
       "137178        0  I bought this pan because my 12 '' pan needed ...\n",
       "233040        0  Just sayin ' . I mean , honestly . Do we reall...\n",
       "413128        1  This rack fits the sink perfectly , and is n't...\n",
       "149988        0  I bought one of the 20X72 inch mats for my hus...\n",
       "18029         1  I have four different sizes of these scoops an...\n",
       "129849        1  These replacement rubbers are perfect for use ..."
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01'),\n",
       " Synset('frump.n.01'),\n",
       " Synset('dog.n.03'),\n",
       " Synset('cad.n.01'),\n",
       " Synset('frank.n.02'),\n",
       " Synset('pawl.n.01'),\n",
       " Synset('andiron.n.01'),\n",
       " Synset('chase.v.01')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dog') # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('chase.v.01')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dog', pos=wn.VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('dog.n.01')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('dog.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n"
     ]
    }
   ],
   "source": [
    "print(wn.synset('dog.n.01').definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wn.synset('dog.n.01').examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dog barked all night\n"
     ]
    }
   ],
   "source": [
    "print(wn.synset('dog.n.01').examples()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('dog.n.01.dog'),\n",
       " Lemma('dog.n.01.domestic_dog'),\n",
       " Lemma('dog.n.01.Canis_familiaris')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('dog.n.01').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'domestic_dog', 'Canis_familiaris']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(lemma.name()) for lemma in wn.synset('dog.n.01').lemmas()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('dog.n.01')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('dog.n.01.dog').synset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\vwslz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle(adversarial_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def alter_wordnet_antonyms(token):\n",
    "#     if token[1] not in POS_TO_WORDNET: return None\n",
    "#     w = token[0].lower()\n",
    "#     wn_pos = POS_TO_WORDNET[token[1]]\n",
    "#     synsets = wn.synsets(w, wn_pos)\n",
    "#     if not synsets: return None\n",
    "#     synset = synsets[0]\n",
    "#     synonyms = []\n",
    "#     antonyms = []\n",
    "\n",
    "#     for syn in wn.synsets(w, wn_pos):\n",
    "#         for lem in syn.lemmas():\n",
    "#             if (lem.name() != w):\n",
    "#                 synonyms.append(lem.name())\n",
    "# #     for lem in synset.lemmas():\n",
    "# #         if lem.antonyms():\n",
    "# # #                 print(\"nouns: \", i)\n",
    "# #             for a in lem.antonyms():\n",
    "# #                 new_word = a.name()\n",
    "# #                 if '_' in a.name(): continue\n",
    "# #                 antonyms.append(new_word)\n",
    "# #         else:\n",
    "# #             print(\"adj: \", i)\n",
    "# #         synonyms.append(lem.name())\n",
    "# #     if token[1] == 'NN':\n",
    "# #         return antonyms\n",
    "# #     else:\n",
    "#     return synonyms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
