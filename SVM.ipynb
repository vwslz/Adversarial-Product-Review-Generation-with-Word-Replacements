{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.callbacks import LambdaCallback\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = \"rawdata_100.pkl\"\n",
    "modelname = 'SVM.h5'\n",
    "max_features = 4000\n",
    "\n",
    "dim_output = 2\n",
    "\n",
    "size_batch = 10\n",
    "num_epoch = 100\n",
    "l2_lambda = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(dataname)\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "tokenizer = RegexpTokenizer(\"[a-z']+\")\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [stemmer.stem(t) for t in tokens] \n",
    "\n",
    "def get_tf(data, use_idf, max_df=1.0, min_df=1, ngram_range=(1,1)):\n",
    "    if use_idf:\n",
    "        m = TfidfVectorizer(\n",
    "            max_df=max_df,\n",
    "            min_df=min_df, \n",
    "            stop_words='english', \n",
    "            ngram_range=ngram_range, \n",
    "            tokenizer=tokenize, \n",
    "            analyzer='word', \n",
    "            max_features = max_features,\n",
    "        )\n",
    "    else:\n",
    "        m = CountVectorizer(max_df=max_df, min_df=min_df, stop_words='english', ngram_range=ngram_range, tokenizer=tokenize)\n",
    "    \n",
    "    d = m.fit_transform(data)\n",
    "    return m, d\n",
    "\n",
    "# tf_m, tf_d = get_tf(data['reviewText'], use_idf=False, max_df=0.90, min_df=10)\n",
    "tfidf_m, tfidf_d = get_tf(data['reviewText'], use_idf=True, max_df=0.90, min_df=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(filename):\n",
    "    print(\"---------------\")\n",
    "    print(\"| Getting data...\")\n",
    "    print(\"---------------\")\n",
    "    \n",
    "    data_X = tfidf_d\n",
    "#     data_Y = np_utils.to_categorical(data['overall'], dim_output)\n",
    "    data_Y = data.overall\n",
    "    return data_X, data_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(x, y):\n",
    "    X_trains, X_test, Y_trains, Y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    X_train, X_validation, Y_train, Y_validation = train_test_split(X_trains, Y_trains, test_size=0.2, random_state=42)\n",
    "    return X_train, X_validation, X_test, Y_train, Y_validation, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "def getParameter(x_train, y_train):\n",
    "    \n",
    "    str(datetime.now())\n",
    "    \n",
    "    tuned_parameters = [{'kernel': ['linear'], 'C': [1, 10]}]\n",
    "    score='accuracy'\n",
    "\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, cv=2, scoring=score, verbose = True)\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print(clf.best_params_)\n",
    "    \n",
    "    str(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def train(x_train, y_train):\n",
    "    print(\"---------------\")\n",
    "    print(\"| Training...\")\n",
    "    print(\"---------------\")\n",
    "    \n",
    "    print(str(datetime.now()))\n",
    "    \n",
    "#     model=svm.SVC(gamma=0.01,C=100.,decision_function_shape='ovo')\n",
    "    model = svm.LinearSVC(verbose = True)\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    print(str(datetime.now()))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, x_test, y_test):\n",
    "    print(\"---------------\")\n",
    "    print(\"| Testing...\")\n",
    "    print(\"---------------\")\n",
    "    \n",
    "    print(str(datetime.now()))\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(accuracy)\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    \n",
    "    print(str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "| Getting data...\n",
      "---------------\n",
      "---------------\n",
      "| Training...\n",
      "---------------\n",
      "2018-06-17 21:31:58.106580\n",
      "[LibLinear]2018-06-17 21:31:59.588615\n",
      "---------------\n",
      "| Testing...\n",
      "---------------\n",
      "2018-06-17 21:31:59.589613\n",
      "0.867658498638662\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.87      0.87     10330\n",
      "          1       0.87      0.87      0.87     10238\n",
      "\n",
      "avg / total       0.87      0.87      0.87     20568\n",
      "\n",
      "2018-06-17 21:31:59.602606\n"
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "X, Y = getData(dataname)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "model = train(X_train,Y_train)\n",
    "test(model, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversasial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getX(x_test):\n",
    "    return tfidf_m.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "| Testing...\n",
      "---------------\n",
      "2018-06-17 21:36:22.975949\n",
      "0.49347385646380426\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.66      0.57     51319\n",
      "          1       0.49      0.32      0.39     51345\n",
      "\n",
      "avg / total       0.49      0.49      0.48    102664\n",
      "\n",
      "2018-06-17 21:36:23.052745\n"
     ]
    }
   ],
   "source": [
    "filename_adv = \"data_adversial_wordnet.pkl\"\n",
    "data = pd.read_pickle(filename_adv)\n",
    "X_test_adv = tfidf_m.fit_transform(data['reviewText'])\n",
    "Y_test_adv = data['overall']\n",
    "test(model, X_test_adv, Y_test_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load('X_test.npy')\n",
    "X_test_adv_responding = getX(X_test)\n",
    "Y_test_adv_responding = np.load('Y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.51390509529366\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_pred_adv_responding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-f3287ae590cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test_adv_responding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_pred_adv_responding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test_adv_responding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_adv_responding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred_adv_responding' is not defined"
     ]
    }
   ],
   "source": [
    "Y_pred_adv_responding = model.predict(X_test_adv_responding)\n",
    "accuracy = accuracy_score(Y_test_adv_responding, Y_pred_adv_responding)\n",
    "print(accuracy)\n",
    "print(classification_report(Y_test_adv_responding, y_pred_adv_responding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, X_test_adv_responding, Y_test_adv_responding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
