{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"rawdata_100.pkl\"]\n",
    "t = Tokenizer(num_words=20000)\n",
    "batch_size = 10\n",
    "l2_lambda = 0.000001\n",
    "num_class = 2\n",
    "\n",
    "randomShuffle = ShuffleSplit(n_splits=1, test_size=.2, random_state=0)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXY(filename_data, num_class):\n",
    "    data = pd.read_pickle(filename_data)\n",
    "    print(\"data: \", data.shape)\n",
    "    reviewTextList = data.reviewText.values\n",
    "    t.fit_on_texts(reviewTextList)\n",
    "    text = t.texts_to_matrix(reviewTextList, mode='count')\n",
    "    \n",
    "    data_X = text\n",
    "    data_Y = data['overall']\n",
    "    data_Y = np_utils.to_categorical(data_Y, num_class)\n",
    "    return data_X, data_Y, text.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVMModel(num_class, input_dim, x_train, x_test, y_train, y_test, batch_size, epochs, name_model):\n",
    "    print(\"x_train: \", x_train.shape)\n",
    "    print(\"y_train: \", y_train.shape)\n",
    "    print(\"x_test: \", x_test.shape)\n",
    "    print(\"y_test: \", y_test.shape)\n",
    "#     print(\"x_train:\",x_train[0])\n",
    "#     print(\"x_test:\",x_test[0])\n",
    "#     print('y_train:',y_train[0])\n",
    "#     print('y_test',y_test[0])\n",
    "    model = Sequential()\n",
    "    model.add(Dense(2,input_dim=input_dim,activation='softmax', kernel_regularizer=l2(l2_lambda)))\n",
    "    print(model.summary())\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n",
    "    history=model.fit(x_train,y_train,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      validation_data=(x_test,y_test),\n",
    "                      )\n",
    "    score=model.evaluate(x_test,y_test)\n",
    "    print('Test loss:',score[0])\n",
    "    print('Test accuracy:',score[1])\n",
    "    plt.figure('categorical_accuracy')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Categorical Accuracy')\n",
    "    plt.plot(history.history['categorical_accuracy'])\n",
    "    plt.show()\n",
    "    y_test=np.argmax(y_test,axis=1)\n",
    "    y_pred=model.predict_classes(x_test)\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    \n",
    "    model.save(name_model)\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "rawdata_100.pkl:\n",
      "data:  (102838, 9)\n",
      "input dimension:  20000\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-2f2fd3afeba7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetXY\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"input dimension: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mSVMModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SVM.H5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2057\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2058\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[1;32m-> 2059\u001b[1;33m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[0;32m   2060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2061\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2057\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2058\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[1;32m-> 2059\u001b[1;33m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[0;32m   2060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2061\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36msafe_indexing\u001b[1;34m(X, indices)\u001b[0m\n\u001b[0;32m    158\u001b[0m                                    indices.dtype.kind == 'i'):\n\u001b[0;32m    159\u001b[0m             \u001b[1;31m# This is often substantially faster than X[indices]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for filename in filenames:\n",
    "    print(\"-----------------------------\")\n",
    "    print(filename + \":\")\n",
    "    X, Y, input_dim = getXY(filename, num_class)\n",
    "    print(\"input dimension: \", input_dim)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    SVMModel(num_class, input_dim, X_train, X_test, Y_train, Y_test, batch_size, 10, 'SVM.H5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('SVM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The upper only 20000 cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ussing adversial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_adv = \"data_adversial_wordnet.pkl\"\n",
    "num_class = 2\n",
    "model = load_model('SVM.h5')\n",
    "X_test_adv, Y_test_adv, input_dim_test_adv = getXY(filename_adv, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102664/102664 [==============================] - 68s 661us/step\n",
      "Test loss: 1.91724966986\n",
      "Test accuracy: 0.472151874075\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.47      0.51      0.49     51319\n",
      "          1       0.47      0.44      0.45     51345\n",
      "\n",
      "avg / total       0.47      0.47      0.47    102664\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score=model.evaluate(X_test_adv,Y_test_adv)\n",
    "print('Test loss:',score[0])\n",
    "print('Test accuracy:',score[1])\n",
    "Y_test_adv=np.argmax(Y_test_adv,axis=1)\n",
    "Y_pred_adv=model.predict_classes(X_test_adv)\n",
    "print(classification_report(Y_test_adv,Y_pred_adv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
