{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Reshape, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"Home_and_Kitchen_5.json\"]\n",
    "filename = \"Home_and_Kitchen_5.json\"\n",
    "t = Tokenizer()\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "\n",
    "dictionary_size = 0\n",
    "embed_dim = 300\n",
    "num_hidden_unit_lstm = 300\n",
    "num_filter = 100\n",
    "window_size = 3\n",
    "pool_size = 2\n",
    "batch_size = 10\n",
    "learning_rate = 0.1\n",
    "#AdaDelta\n",
    "dropout_embed = 0.5\n",
    "dropout_blstm = 0.2\n",
    "dropout_penultimate = 0.4\n",
    "l2_lambda = 0.000001\n",
    "\n",
    "randomShuffle = ShuffleSplit(n_splits=1, test_size=.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(filename, lines=True, orient=\"frame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0615391206</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>My daughter wanted this book and the price on ...</td>\n",
       "      <td>10 19, 2013</td>\n",
       "      <td>APYOBQE6M18AA</td>\n",
       "      <td>Martin Schwartz</td>\n",
       "      <td>Best Price</td>\n",
       "      <td>1382140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0615391206</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>I bought this zoku quick pop for my daughterr ...</td>\n",
       "      <td>06 18, 2014</td>\n",
       "      <td>A1JVQTAGHYOL7F</td>\n",
       "      <td>Michelle Dinh</td>\n",
       "      <td>zoku</td>\n",
       "      <td>1403049600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0615391206</td>\n",
       "      <td>[26, 27]</td>\n",
       "      <td>4</td>\n",
       "      <td>There is no shortage of pop recipes available ...</td>\n",
       "      <td>05 5, 2013</td>\n",
       "      <td>A3UPYGJKZ0XTU4</td>\n",
       "      <td>mirasreviews</td>\n",
       "      <td>Excels at Sweet Dessert Pops, but Falls Short ...</td>\n",
       "      <td>1367712000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0615391206</td>\n",
       "      <td>[14, 18]</td>\n",
       "      <td>5</td>\n",
       "      <td>This book is a must have if you get a Zoku (wh...</td>\n",
       "      <td>08 4, 2011</td>\n",
       "      <td>A2MHCTX43MIMDZ</td>\n",
       "      <td>M. Johnson \"Tea Lover\"</td>\n",
       "      <td>Creative Combos</td>\n",
       "      <td>1312416000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0615391206</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>4</td>\n",
       "      <td>This cookbook is great.  I have really enjoyed...</td>\n",
       "      <td>06 7, 2014</td>\n",
       "      <td>AHAI85T5C2DH3</td>\n",
       "      <td>PugLover</td>\n",
       "      <td>A must own if you own the Zoku maker...</td>\n",
       "      <td>1402099200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0615391206</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>If you have a Zoku Quick Pop maker (or two.......</td>\n",
       "      <td>06 7, 2014</td>\n",
       "      <td>AXA9EVY6IJIZ5</td>\n",
       "      <td>Robin Cline \"Solarobby\"</td>\n",
       "      <td>Love it</td>\n",
       "      <td>1402099200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0615391206</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>5</td>\n",
       "      <td>This book is so beautifully illustrated and ea...</td>\n",
       "      <td>07 2, 2012</td>\n",
       "      <td>A1SW2D234X11MS</td>\n",
       "      <td>savinggrace \"savinggrace\"</td>\n",
       "      <td>Beautifully illustrated recipe and instruction...</td>\n",
       "      <td>1341187200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0615391206</td>\n",
       "      <td>[32, 39]</td>\n",
       "      <td>2</td>\n",
       "      <td>I bought this when I bought the pop maker. I t...</td>\n",
       "      <td>12 8, 2011</td>\n",
       "      <td>A1ZH5ULI4SBO48</td>\n",
       "      <td>S. G. \"s.g.\"</td>\n",
       "      <td>Okay but you can figure it out yourself withou...</td>\n",
       "      <td>1323302400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0615391206</td>\n",
       "      <td>[13, 13]</td>\n",
       "      <td>5</td>\n",
       "      <td>The Zoku accessories are pricey, but the recip...</td>\n",
       "      <td>09 22, 2011</td>\n",
       "      <td>A1V13XAWO98C1J</td>\n",
       "      <td>Stan \"Stan\"</td>\n",
       "      <td>If you have the Zoku maker, buy this!</td>\n",
       "      <td>1316649600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0615391206</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>High quality book. Clear and helpful informati...</td>\n",
       "      <td>06 1, 2014</td>\n",
       "      <td>A3842PXNQ1QRM9</td>\n",
       "      <td>Stephen Brem</td>\n",
       "      <td>Satisfaction opinion</td>\n",
       "      <td>1401580800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin   helpful  overall  \\\n",
       "0  0615391206    [0, 0]        5   \n",
       "1  0615391206    [0, 0]        5   \n",
       "2  0615391206  [26, 27]        4   \n",
       "3  0615391206  [14, 18]        5   \n",
       "4  0615391206    [0, 0]        4   \n",
       "5  0615391206    [0, 0]        5   \n",
       "6  0615391206    [2, 3]        5   \n",
       "7  0615391206  [32, 39]        2   \n",
       "8  0615391206  [13, 13]        5   \n",
       "9  0615391206    [0, 0]        5   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  My daughter wanted this book and the price on ...  10 19, 2013   \n",
       "1  I bought this zoku quick pop for my daughterr ...  06 18, 2014   \n",
       "2  There is no shortage of pop recipes available ...   05 5, 2013   \n",
       "3  This book is a must have if you get a Zoku (wh...   08 4, 2011   \n",
       "4  This cookbook is great.  I have really enjoyed...   06 7, 2014   \n",
       "5  If you have a Zoku Quick Pop maker (or two.......   06 7, 2014   \n",
       "6  This book is so beautifully illustrated and ea...   07 2, 2012   \n",
       "7  I bought this when I bought the pop maker. I t...   12 8, 2011   \n",
       "8  The Zoku accessories are pricey, but the recip...  09 22, 2011   \n",
       "9  High quality book. Clear and helpful informati...   06 1, 2014   \n",
       "\n",
       "       reviewerID               reviewerName  \\\n",
       "0   APYOBQE6M18AA            Martin Schwartz   \n",
       "1  A1JVQTAGHYOL7F              Michelle Dinh   \n",
       "2  A3UPYGJKZ0XTU4               mirasreviews   \n",
       "3  A2MHCTX43MIMDZ     M. Johnson \"Tea Lover\"   \n",
       "4   AHAI85T5C2DH3                   PugLover   \n",
       "5   AXA9EVY6IJIZ5    Robin Cline \"Solarobby\"   \n",
       "6  A1SW2D234X11MS  savinggrace \"savinggrace\"   \n",
       "7  A1ZH5ULI4SBO48               S. G. \"s.g.\"   \n",
       "8  A1V13XAWO98C1J                Stan \"Stan\"   \n",
       "9  A3842PXNQ1QRM9               Stephen Brem   \n",
       "\n",
       "                                             summary  unixReviewTime  \n",
       "0                                         Best Price      1382140800  \n",
       "1                                               zoku      1403049600  \n",
       "2  Excels at Sweet Dessert Pops, but Falls Short ...      1367712000  \n",
       "3                                    Creative Combos      1312416000  \n",
       "4            A must own if you own the Zoku maker...      1402099200  \n",
       "5                                            Love it      1402099200  \n",
       "6  Beautifully illustrated recipe and instruction...      1341187200  \n",
       "7  Okay but you can figure it out yourself withou...      1323302400  \n",
       "8              If you have the Zoku maker, buy this!      1316649600  \n",
       "9                               Satisfaction opinion      1401580800  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['overall'] = np.where(data['overall'] < 3, 0, data.overall)\n",
    "data['overall'] = np.where(data['overall'] == 3, 1, data.overall)\n",
    "data['overall'] = np.where(data['overall'] > 3, 2, data.overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0615391206</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>2</td>\n",
       "      <td>My daughter wanted this book and the price on ...</td>\n",
       "      <td>10 19, 2013</td>\n",
       "      <td>APYOBQE6M18AA</td>\n",
       "      <td>Martin Schwartz</td>\n",
       "      <td>Best Price</td>\n",
       "      <td>1382140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0615391206</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>2</td>\n",
       "      <td>I bought this zoku quick pop for my daughterr ...</td>\n",
       "      <td>06 18, 2014</td>\n",
       "      <td>A1JVQTAGHYOL7F</td>\n",
       "      <td>Michelle Dinh</td>\n",
       "      <td>zoku</td>\n",
       "      <td>1403049600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0615391206</td>\n",
       "      <td>[26, 27]</td>\n",
       "      <td>2</td>\n",
       "      <td>There is no shortage of pop recipes available ...</td>\n",
       "      <td>05 5, 2013</td>\n",
       "      <td>A3UPYGJKZ0XTU4</td>\n",
       "      <td>mirasreviews</td>\n",
       "      <td>Excels at Sweet Dessert Pops, but Falls Short ...</td>\n",
       "      <td>1367712000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0615391206</td>\n",
       "      <td>[14, 18]</td>\n",
       "      <td>2</td>\n",
       "      <td>This book is a must have if you get a Zoku (wh...</td>\n",
       "      <td>08 4, 2011</td>\n",
       "      <td>A2MHCTX43MIMDZ</td>\n",
       "      <td>M. Johnson \"Tea Lover\"</td>\n",
       "      <td>Creative Combos</td>\n",
       "      <td>1312416000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0615391206</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>2</td>\n",
       "      <td>This cookbook is great.  I have really enjoyed...</td>\n",
       "      <td>06 7, 2014</td>\n",
       "      <td>AHAI85T5C2DH3</td>\n",
       "      <td>PugLover</td>\n",
       "      <td>A must own if you own the Zoku maker...</td>\n",
       "      <td>1402099200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0615391206</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>2</td>\n",
       "      <td>If you have a Zoku Quick Pop maker (or two.......</td>\n",
       "      <td>06 7, 2014</td>\n",
       "      <td>AXA9EVY6IJIZ5</td>\n",
       "      <td>Robin Cline \"Solarobby\"</td>\n",
       "      <td>Love it</td>\n",
       "      <td>1402099200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0615391206</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>2</td>\n",
       "      <td>This book is so beautifully illustrated and ea...</td>\n",
       "      <td>07 2, 2012</td>\n",
       "      <td>A1SW2D234X11MS</td>\n",
       "      <td>savinggrace \"savinggrace\"</td>\n",
       "      <td>Beautifully illustrated recipe and instruction...</td>\n",
       "      <td>1341187200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0615391206</td>\n",
       "      <td>[32, 39]</td>\n",
       "      <td>0</td>\n",
       "      <td>I bought this when I bought the pop maker. I t...</td>\n",
       "      <td>12 8, 2011</td>\n",
       "      <td>A1ZH5ULI4SBO48</td>\n",
       "      <td>S. G. \"s.g.\"</td>\n",
       "      <td>Okay but you can figure it out yourself withou...</td>\n",
       "      <td>1323302400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0615391206</td>\n",
       "      <td>[13, 13]</td>\n",
       "      <td>2</td>\n",
       "      <td>The Zoku accessories are pricey, but the recip...</td>\n",
       "      <td>09 22, 2011</td>\n",
       "      <td>A1V13XAWO98C1J</td>\n",
       "      <td>Stan \"Stan\"</td>\n",
       "      <td>If you have the Zoku maker, buy this!</td>\n",
       "      <td>1316649600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0615391206</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>2</td>\n",
       "      <td>High quality book. Clear and helpful informati...</td>\n",
       "      <td>06 1, 2014</td>\n",
       "      <td>A3842PXNQ1QRM9</td>\n",
       "      <td>Stephen Brem</td>\n",
       "      <td>Satisfaction opinion</td>\n",
       "      <td>1401580800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin   helpful  overall  \\\n",
       "0  0615391206    [0, 0]        2   \n",
       "1  0615391206    [0, 0]        2   \n",
       "2  0615391206  [26, 27]        2   \n",
       "3  0615391206  [14, 18]        2   \n",
       "4  0615391206    [0, 0]        2   \n",
       "5  0615391206    [0, 0]        2   \n",
       "6  0615391206    [2, 3]        2   \n",
       "7  0615391206  [32, 39]        0   \n",
       "8  0615391206  [13, 13]        2   \n",
       "9  0615391206    [0, 0]        2   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  My daughter wanted this book and the price on ...  10 19, 2013   \n",
       "1  I bought this zoku quick pop for my daughterr ...  06 18, 2014   \n",
       "2  There is no shortage of pop recipes available ...   05 5, 2013   \n",
       "3  This book is a must have if you get a Zoku (wh...   08 4, 2011   \n",
       "4  This cookbook is great.  I have really enjoyed...   06 7, 2014   \n",
       "5  If you have a Zoku Quick Pop maker (or two.......   06 7, 2014   \n",
       "6  This book is so beautifully illustrated and ea...   07 2, 2012   \n",
       "7  I bought this when I bought the pop maker. I t...   12 8, 2011   \n",
       "8  The Zoku accessories are pricey, but the recip...  09 22, 2011   \n",
       "9  High quality book. Clear and helpful informati...   06 1, 2014   \n",
       "\n",
       "       reviewerID               reviewerName  \\\n",
       "0   APYOBQE6M18AA            Martin Schwartz   \n",
       "1  A1JVQTAGHYOL7F              Michelle Dinh   \n",
       "2  A3UPYGJKZ0XTU4               mirasreviews   \n",
       "3  A2MHCTX43MIMDZ     M. Johnson \"Tea Lover\"   \n",
       "4   AHAI85T5C2DH3                   PugLover   \n",
       "5   AXA9EVY6IJIZ5    Robin Cline \"Solarobby\"   \n",
       "6  A1SW2D234X11MS  savinggrace \"savinggrace\"   \n",
       "7  A1ZH5ULI4SBO48               S. G. \"s.g.\"   \n",
       "8  A1V13XAWO98C1J                Stan \"Stan\"   \n",
       "9  A3842PXNQ1QRM9               Stephen Brem   \n",
       "\n",
       "                                             summary  unixReviewTime  \n",
       "0                                         Best Price      1382140800  \n",
       "1                                               zoku      1403049600  \n",
       "2  Excels at Sweet Dessert Pops, but Falls Short ...      1367712000  \n",
       "3                                    Creative Combos      1312416000  \n",
       "4            A must own if you own the Zoku maker...      1402099200  \n",
       "5                                            Love it      1402099200  \n",
       "6  Beautifully illustrated recipe and instruction...      1341187200  \n",
       "7  Okay but you can figure it out yourself withou...      1323302400  \n",
       "8              If you have the Zoku maker, buy this!      1316649600  \n",
       "9                               Satisfaction opinion      1401580800  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 51419\t\t9.32%\n",
      "1: 45059\t\t8.17%\n",
      "2: 455204\t\t82.51%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label = list(data.iloc[:, 2].values)\n",
    "for j in range(3):\n",
    "    print(str(j) + \": \" + str(label.count(j)) + \"\\t\\t\" + str(round(label.count(j) / len(label) * 100, 2)) + \"%\")\n",
    "\n",
    "plt.plot([0, 1, 2], [label.count(0), label.count(1), label.count(2)], 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingMatrix(vocabulary_size):\n",
    "    embeddings_index = dict()\n",
    "    f = open('glove.6b/glove.6B.300d.txt', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    embedding_matrix = np.zeros((vocabulary_size, embed_dim))\n",
    "    for word, i in t.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 classes(0: 1-2 star, 1: 4-5 star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXY(filename, num_class):\n",
    "    data = pd.read_json(filename, lines=True, orient=\"frame\")\n",
    "    data['overall'] = np.where(data['overall'] < 3, 0, data.overall)\n",
    "    data['overall'] = np.where(data['overall'] > 3, 1, data.overall)\n",
    "    \n",
    "    # down sample\n",
    "    data_0 = data.loc[data['overall'] == 0]\n",
    "    data_1 = data.loc[data['overall'] == 1].sample(len(data_0))\n",
    "    \n",
    "    data = data_0.append(data_1).sample(frac=1)\n",
    "    \n",
    "    good_columns = [3]\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    data_X = data.iloc[:, good_columns]\n",
    "    data_Y = data.iloc[:, 2]\n",
    "    \n",
    "    reviewTextList = data.reviewText.values\n",
    "    t.fit_on_texts(reviewTextList)\n",
    "    text = t.texts_to_sequences(reviewTextList)\n",
    "    text = sequence.pad_sequences(text, maxlen=maxlen)\n",
    "    print('text shape:', text.shape)\n",
    "    vocabulary_size = len(t.word_index) + 1\n",
    "    print('vocabulary size:', vocabulary_size)\n",
    "    \n",
    "    data_X = np.array(text)\n",
    "    data_Y = np_utils.to_categorical(data_Y, num_class)\n",
    "    return data_X, data_Y, vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectionalLSTMCNNModel(containsGlove, num_class, str_loss, x_train, x_test, y_train, y_test, batch_size, epochs, vocabulary_size, embedding_matrix, name_model):\n",
    "    print(\"x_train: \", x_train.shape)\n",
    "    print(\"y_train: \", y_train.shape)\n",
    "    print(\"x_test: \", x_test.shape)\n",
    "    print(\"y_test: \", y_test.shape)\n",
    "#     print(\"x_train:\",x_train[0])\n",
    "#     print(\"x_test:\",x_test[0])\n",
    "#     print('y_train:',y_train[0])\n",
    "#     print('y_test',y_test[0])\n",
    "    model = Sequential()\n",
    "    print(\"vocabulary_size:\", vocabulary_size)\n",
    "    if (containsGlove == False):\n",
    "        model.add(Embedding(input_dim=vocabulary_size, output_dim=embed_dim, input_length=maxlen))\n",
    "    else:\n",
    "        model.add(Embedding(input_dim=vocabulary_size, output_dim=embed_dim, input_length=maxlen, weights=[embedding_matrix], trainable=False))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            LSTM(\n",
    "                num_hidden_unit_lstm, \n",
    "                return_sequences=True, \n",
    "                 kernel_regularizer=l2(l2_lambda)\n",
    "            ), \n",
    "            merge_mode=\"sum\"\n",
    "        ))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Reshape((maxlen, embed_dim, 1)))\n",
    "    model.add(Conv2D(num_filter,\n",
    "                     kernel_size=(3,3),\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1,\n",
    "                      kernel_regularizer=l2(l2_lambda)\n",
    "                    ))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_class, kernel_regularizer=l2(l2_lambda)))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    print(model.summary())\n",
    "    model.compile(loss=str_loss, \n",
    "                  optimizer='Adadelta',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print('Train...')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=[x_test, y_test])\n",
    "    \n",
    "    score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    print('Test score:', score)\n",
    "    print('Test accuracy:', acc)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    y_pred = model.predict_classes(x_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    model.save(name_model)\n",
    "    del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare without and with GloVe Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Home_and_Kitchen_5.json:\n",
      "text shape: (102838, 100)\n",
      "vocabulary size: 72649\n",
      "Loaded 400000 word vectors.\n",
      "x_train:  (82270, 100)\n",
      "y_train:  (82270, 2)\n",
      "x_test:  (20568, 100)\n",
      "y_test:  (20568, 2)\n",
      "vocabulary_size: 72649\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          21794700  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 300)          1442400   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 100, 300, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 98, 298, 100)      1000      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 49, 149, 100)      0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 49, 149, 100)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 730100)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1460202   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 24,698,302\n",
      "Trainable params: 24,698,302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train...\n",
      "Train on 82270 samples, validate on 20568 samples\n",
      "Epoch 1/1\n",
      "82270/82270 [==============================] - 3357s 41ms/step - loss: 0.3445 - acc: 0.8422 - val_loss: 0.2613 - val_acc: 0.8894\n",
      "20568/20568 [==============================] - 251s 12ms/step\n",
      "Test score: 0.2612825062097666\n",
      "Test accuracy: 0.8893669706195859\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.94      0.89     10248\n",
      "          1       0.93      0.84      0.88     10320\n",
      "\n",
      "avg / total       0.89      0.89      0.89     20568\n",
      "\n",
      "x_train:  (82270, 100)\n",
      "y_train:  (82270, 2)\n",
      "x_test:  (20568, 100)\n",
      "y_test:  (20568, 2)\n",
      "vocabulary_size: 72649\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 300)          21794700  \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 100, 300)          1442400   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 100, 300, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 98, 298, 100)      1000      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 49, 149, 100)      0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 49, 149, 100)      0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 730100)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1460202   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 24,698,302\n",
      "Trainable params: 2,903,602\n",
      "Non-trainable params: 21,794,700\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train...\n",
      "Train on 82270 samples, validate on 20568 samples\n",
      "Epoch 1/1\n",
      "82270/82270 [==============================] - 2917s 35ms/step - loss: 0.3934 - acc: 0.8224 - val_loss: 0.3200 - val_acc: 0.8733\n",
      "20568/20568 [==============================] - 252s 12ms/step\n",
      "Test score: 0.32004917558581103\n",
      "Test accuracy: 0.8732740111918544\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.81      0.86     10248\n",
      "          1       0.83      0.94      0.88     10320\n",
      "\n",
      "avg / total       0.88      0.87      0.87     20568\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 0\n",
    "num_class = 2\n",
    "for filename in filenames:\n",
    "    print(\"-----------------------------\")\n",
    "    print(filename + \":\")\n",
    "    X, Y, vocabulary_size = getXY(filename, num_class)\n",
    "    embedding_matrix = getEmbeddingMatrix(vocabulary_size)\n",
    "    for train_index, test_index in randomShuffle.split(X):\n",
    "        X_train = X[train_index]\n",
    "        Y_train = Y[train_index, :]\n",
    "        X_test = X[test_index]\n",
    "        Y_test = Y[test_index, :]\n",
    "        \n",
    "        bidirectionalLSTMCNNModel(False, num_class, 'binary_crossentropy', X_train, X_test, Y_train, Y_test, 10, 1, vocabulary_size, embedding_matrix, filename + 'BLSTM+CNN_without.h5')\n",
    "        bidirectionalLSTMCNNModel(True, num_class, 'binary_crossentropy', X_train, X_test, Y_train, Y_test, 10, 1, vocabulary_size, embedding_matrix, filename + 'BLSTM+CNN_with.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run without GloVe for 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Home_and_Kitchen_5.json:\n",
      "text shape: (102838, 100)\n",
      "vocabulary size: 72975\n",
      "Loaded 400000 word vectors.\n",
      "x_train:  (82270, 100)\n",
      "y_train:  (82270, 2)\n",
      "x_test:  (20568, 100)\n",
      "y_test:  (20568, 2)\n",
      "vocabulary_size: 72975\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          21892500  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 300)          1442400   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 100, 300, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 98, 298, 100)      1000      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 49, 149, 100)      0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 49, 149, 100)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 730100)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1460202   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 24,796,102\n",
      "Trainable params: 24,796,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train...\n",
      "Train on 82270 samples, validate on 20568 samples\n",
      "Epoch 1/5\n",
      "45550/82270 [===============>..............] - ETA: 23:45 - loss: 0.3746 - acc: 0.8259"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-a0b065e6bfc7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mbidirectionalLSTMCNNModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'BLSTM+CNN.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-d78c061e9655>\u001b[0m in \u001b[0;36mbidirectionalLSTMCNNModel\u001b[1;34m(containsGlove, num_class, str_loss, x_train, x_test, y_train, y_test, batch_size, epochs, vocabulary_size, embedding_matrix, name_model)\u001b[0m\n\u001b[0;32m     48\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m               validation_data=[x_test, y_test])\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vocabulary_size = 0\n",
    "num_class = 2\n",
    "for filename in filenames:\n",
    "    print(\"-----------------------------\")\n",
    "    print(filename + \":\")\n",
    "    X, Y, vocabulary_size = getXY(filename, num_class)\n",
    "    embedding_matrix = getEmbeddingMatrix(vocabulary_size)\n",
    "    for train_index, test_index in randomShuffle.split(X):\n",
    "        X_train = X[train_index]\n",
    "        Y_train = Y[train_index, :]\n",
    "        X_test = X[test_index]\n",
    "        Y_test = Y[test_index, :]\n",
    "        \n",
    "        bidirectionalLSTMCNNModel(False, num_class, 'categorical_crossentropy', X_train, X_test, Y_train, Y_test, 10, 5, vocabulary_size, embedding_matrix, 'BLSTM+CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('BLSTM+CNN.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
